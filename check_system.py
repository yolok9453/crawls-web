#!/usr/bin/env python3
"""
æª¢æŸ¥çˆ¬èŸ²ç³»çµ±çš„å®Œæ•´æµç¨‹å’Œå•é¡Œ
"""

import os
import sys
import sqlite3
import requests
import importlib.util
from datetime import datetime

# æ·»åŠ å°ˆæ¡ˆæ ¹ç›®éŒ„åˆ°Pythonè·¯å¾‘
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)
sys.path.insert(0, os.path.join(project_root, 'app'))
sys.path.insert(0, os.path.join(project_root, 'core'))

def check_file_structure():
    """æª¢æŸ¥æ–‡ä»¶çµæ§‹"""
    print("=== æª¢æŸ¥æ–‡ä»¶çµæ§‹ ===")
    
    required_files = [
        'main.py',
        'app/web_app.py',
        'core/database.py',
        'core/crawler_manager.py',
        'config/requirements.txt',
        'data/crawler_data.db'
    ]
    
    missing_files = []
    for file_path in required_files:
        full_path = os.path.join(project_root, file_path)
        if os.path.exists(full_path):
            print(f"âœ… {file_path}")
        else:
            print(f"âŒ {file_path} (ç¼ºå¤±)")
            missing_files.append(file_path)
    
    return len(missing_files) == 0

def check_database():
    """æª¢æŸ¥è³‡æ–™åº«é€£æ¥å’Œçµæ§‹"""
    print("\n=== æª¢æŸ¥è³‡æ–™åº« ===")
    
    try:
        from core.database import get_db_connection, init_db
        
        # åˆå§‹åŒ–è³‡æ–™åº«
        init_db()
        print("âœ… è³‡æ–™åº«åˆå§‹åŒ–æˆåŠŸ")
        
        # æª¢æŸ¥é€£æ¥
        conn = get_db_connection()
        cursor = conn.cursor()
        
        # æª¢æŸ¥è¡¨æ˜¯å¦å­˜åœ¨
        tables = ['crawl_sessions', 'products', 'daily_deals']
        for table in tables:
            cursor.execute(f"SELECT COUNT(*) FROM {table}")
            count = cursor.fetchone()[0]
            print(f"âœ… {table} è¡¨: {count} ç­†è¨˜éŒ„")
        
        conn.close()
        return True
        
    except Exception as e:
        print(f"âŒ è³‡æ–™åº«æª¢æŸ¥å¤±æ•—: {e}")
        return False

def check_crawlers():
    """æª¢æŸ¥çˆ¬èŸ²æ¨¡çµ„"""
    print("\n=== æª¢æŸ¥çˆ¬èŸ²æ¨¡çµ„ ===")
    
    try:
        from core.crawler_manager import CrawlerManager
        
        manager = CrawlerManager()
        crawlers = manager.list_crawlers()
        
        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(crawlers)} å€‹çˆ¬èŸ²:")
        for crawler in crawlers:
            print(f"  - {crawler}")
        
        return len(crawlers) > 0
        
    except Exception as e:
        print(f"âŒ çˆ¬èŸ²æª¢æŸ¥å¤±æ•—: {e}")
        return False

def check_dependencies():
    """æª¢æŸ¥Pythonä¾è³´å¥—ä»¶"""
    print("\n=== æª¢æŸ¥ä¾è³´å¥—ä»¶ ===")
    
    required_packages = [
        ('flask', 'flask'),
        ('requests', 'requests'),
        ('beautifulsoup4', 'bs4'),
        ('selenium', 'selenium'),
        ('lxml', 'lxml'),
        ('python-dotenv', 'dotenv')
    ]
    
    missing_packages = []
    for package_name, import_name in required_packages:
        try:
            __import__(import_name)
            print(f"âœ… {package_name}")
        except ImportError:
            print(f"âŒ {package_name} (æœªå®‰è£)")
            missing_packages.append(package_name)
    
    # æª¢æŸ¥å¯é¸å¥—ä»¶
    try:
        import google.generativeai as genai
        print("âœ… google-generativeai (AIåŠŸèƒ½å¯ç”¨)")
    except ImportError:
        print("âš ï¸ google-generativeai (AIåŠŸèƒ½å°‡è¢«ç¦ç”¨)")
    
    return len(missing_packages) == 0

def check_environment():
    """æª¢æŸ¥ç’°å¢ƒé…ç½®"""
    print("\n=== æª¢æŸ¥ç’°å¢ƒé…ç½® ===")
    
    # æª¢æŸ¥.envæ–‡ä»¶
    env_path = os.path.join(project_root, 'config', '.env')
    if os.path.exists(env_path):
        print("âœ… .env æ–‡ä»¶å­˜åœ¨")
        
        # æª¢æŸ¥APIå¯†é‘°
        try:
            from dotenv import load_dotenv
            load_dotenv(env_path)
            
            gemini_key = os.getenv('GEMINI_API_KEY')
            if gemini_key:
                print("âœ… GEMINI_API_KEY å·²é…ç½®")
            else:
                print("âš ï¸ GEMINI_API_KEY æœªé…ç½® (AIåŠŸèƒ½å°‡è¢«ç¦ç”¨)")
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•è®€å–.envæ–‡ä»¶: {e}")
    else:
        print("âš ï¸ .env æ–‡ä»¶ä¸å­˜åœ¨")
    
    return True

def check_web_service():
    """æª¢æŸ¥Webæœå‹™"""
    print("\n=== æª¢æŸ¥Webæœå‹™ ===")
    
    try:
        # å˜—è©¦åŒ¯å…¥web_app
        from app.web_app import app
        print("âœ… Flaskæ‡‰ç”¨ç¨‹å¼è¼‰å…¥æˆåŠŸ")
        
        # æª¢æŸ¥è·¯ç”±
        routes = []
        for rule in app.url_map.iter_rules():
            routes.append(str(rule))
        
        print(f"âœ… è¨»å†Šäº† {len(routes)} å€‹è·¯ç”±")
        important_routes = ['/', '/api/results', '/api/daily-deals', '/crawler']
        for route in important_routes:
            if any(route in r for r in routes):
                print(f"  âœ… {route}")
            else:
                print(f"  âŒ {route} (ç¼ºå¤±)")
        
        return True
        
    except Exception as e:
        print(f"âŒ Webæœå‹™æª¢æŸ¥å¤±æ•—: {e}")
        return False

def test_simple_crawler():
    """æ¸¬è©¦ç°¡å–®çš„çˆ¬èŸ²åŠŸèƒ½"""
    print("\n=== æ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½ ===")
    
    try:
        from core.crawler_manager import CrawlerManager
        
        manager = CrawlerManager()
        crawlers = manager.list_crawlers()
        
        if not crawlers:
            print("âŒ æ²’æœ‰å¯ç”¨çš„çˆ¬èŸ²")
            return False
        
        # é¸æ“‡ç¬¬ä¸€å€‹çˆ¬èŸ²é€²è¡Œæ¸¬è©¦
        test_platform = crawlers[0]
        print(f"ğŸ§ª æ¸¬è©¦ {test_platform} çˆ¬èŸ²...")
        
        # åŸ·è¡Œå°è¦æ¨¡æ¸¬è©¦
        result = manager.run_single_crawler(
            platform=test_platform,
            keyword="æ¸¬è©¦å•†å“",
            max_products=3
        )
        
        if result['status'] == 'success':
            print(f"âœ… çˆ¬èŸ²æ¸¬è©¦æˆåŠŸï¼Œç²å¾— {result['total_products']} å€‹å•†å“")
            return True
        else:
            print(f"âŒ çˆ¬èŸ²æ¸¬è©¦å¤±æ•—: {result.get('error', 'æœªçŸ¥éŒ¯èª¤')}")
            return False
            
    except Exception as e:
        print(f"âŒ çˆ¬èŸ²æ¸¬è©¦å¤±æ•—: {e}")
        return False

def generate_recommendations():
    """ç”Ÿæˆä¿®å¾©å»ºè­°"""
    print("\n=== ä¿®å¾©å»ºè­° ===")
    
    recommendations = []
    
    # æª¢æŸ¥.envæ–‡ä»¶
    env_path = os.path.join(project_root, 'config', '.env')
    if not os.path.exists(env_path):
        recommendations.append("1. å‰µå»º config/.env æ–‡ä»¶ä¸¦é…ç½® GEMINI_API_KEY (ç”¨æ–¼AIåŠŸèƒ½)")
    
    # æª¢æŸ¥requirements
    req_path = os.path.join(project_root, 'config', 'requirements.txt')
    if os.path.exists(req_path):
        recommendations.append("2. åŸ·è¡Œ: pip install -r config/requirements.txt")
    
    # æª¢æŸ¥è³‡æ–™åº«
    db_path = os.path.join(project_root, 'data', 'crawler_data.db')
    if not os.path.exists(db_path):
        recommendations.append("3. åˆå§‹åŒ–è³‡æ–™åº«: python core/database.py")
    
    recommendations.extend([
        "4. å•Ÿå‹•Webæœå‹™: python main.py",
        "5. è¨ªå• http://localhost:5000 æŸ¥çœ‹ç•Œé¢",
        "6. æ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½: è¨ªå• /crawler é é¢"
    ])
    
    for rec in recommendations:
        print(f"ğŸ’¡ {rec}")

def main():
    """ä¸»è¦æª¢æŸ¥æµç¨‹"""
    print("ğŸ” é–‹å§‹æª¢æŸ¥çˆ¬èŸ²ç³»çµ±...")
    print(f"ğŸ“ å°ˆæ¡ˆè·¯å¾‘: {project_root}")
    
    checks = [
        ("æ–‡ä»¶çµæ§‹", check_file_structure),
        ("ä¾è³´å¥—ä»¶", check_dependencies),
        ("ç’°å¢ƒé…ç½®", check_environment),
        ("è³‡æ–™åº«", check_database),
        ("çˆ¬èŸ²æ¨¡çµ„", check_crawlers),
        ("Webæœå‹™", check_web_service),
    ]
    
    passed = 0
    total = len(checks)
    
    for name, check_func in checks:
        print(f"\n{'='*20} {name} {'='*20}")
        try:
            if check_func():
                passed += 1
                print(f"âœ… {name} æª¢æŸ¥é€šé")
            else:
                print(f"âŒ {name} æª¢æŸ¥å¤±æ•—")
        except Exception as e:
            print(f"âŒ {name} æª¢æŸ¥ç™¼ç”Ÿç•°å¸¸: {e}")
    
    print(f"\n{'='*50}")
    print(f"ğŸ“Š æª¢æŸ¥çµæœ: {passed}/{total} é …é€šé")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æª¢æŸ¥éƒ½é€šéï¼ç³»çµ±æ‡‰è©²å¯ä»¥æ­£å¸¸é‹è¡Œã€‚")
        
        # é¡å¤–é€²è¡Œçˆ¬èŸ²æ¸¬è©¦
        if input("\næ˜¯å¦è¦é€²è¡Œçˆ¬èŸ²åŠŸèƒ½æ¸¬è©¦ï¼Ÿ(y/N): ").lower() == 'y':
            test_simple_crawler()
    else:
        print("âš ï¸ ç™¼ç¾å•é¡Œï¼Œè«‹æŸ¥çœ‹ä¸Šè¿°æª¢æŸ¥çµæœä¸¦ä¿®å¾©ã€‚")
    
    generate_recommendations()

if __name__ == "__main__":
    main()
