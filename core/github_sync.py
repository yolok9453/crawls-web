"""
GitHub è³‡æ–™åº«åŒæ­¥æ¨¡çµ„
å¾ GitHub å€‰åº«ä¸‹è¼‰æœ€æ–°çš„è³‡æ–™åº«æª”æ¡ˆ
"""

import os
import requests
import shutil
from datetime import datetime

def download_latest_database(github_username="yolok9453", repo_name="crawls-web", branch="master"):
    """
    å¾ GitHub ä¸‹è¼‰æœ€æ–°çš„è³‡æ–™åº«æª”æ¡ˆ
    """
    try:
        # GitHub raw file URL
        db_url = f"https://raw.githubusercontent.com/{github_username}/{repo_name}/{branch}/data/crawler_data.db"
        
        # æœ¬åœ°è³‡æ–™åº«è·¯å¾‘
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        local_db_path = os.path.join(project_root, 'data', 'crawler_data.db')
        backup_db_path = os.path.join(project_root, 'data', f'crawler_data_backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}.db')
        
        # ç¢ºä¿ç›®éŒ„å­˜åœ¨
        os.makedirs(os.path.dirname(local_db_path), exist_ok=True)
        
        print(f"ğŸ”„ æ­£åœ¨å¾ GitHub ä¸‹è¼‰æœ€æ–°è³‡æ–™åº«...")
        print(f"ğŸ“¥ ä¸‹è¼‰ç¶²å€: {db_url}")
        
        # ä¸‹è¼‰æª”æ¡ˆ
        response = requests.get(db_url, timeout=30)
        response.raise_for_status()
        
        # å‚™ä»½ç¾æœ‰è³‡æ–™åº«ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if os.path.exists(local_db_path):
            shutil.copy2(local_db_path, backup_db_path)
            print(f"ğŸ’¾ å·²å‚™ä»½ç¾æœ‰è³‡æ–™åº«åˆ°: {backup_db_path}")
        
        # å„²å­˜æ–°è³‡æ–™åº«
        with open(local_db_path, 'wb') as f:
            f.write(response.content)
        
        print(f"âœ… æˆåŠŸä¸‹è¼‰è³‡æ–™åº«åˆ°: {local_db_path}")
        print(f"ğŸ“Š æª”æ¡ˆå¤§å°: {len(response.content)} bytes")
        
        return True
        
    except requests.exceptions.RequestException as e:
        print(f"âŒ ä¸‹è¼‰å¤±æ•— - ç¶²è·¯éŒ¯èª¤: {e}")
        return False
    except Exception as e:
        print(f"âŒ ä¸‹è¼‰å¤±æ•— - å…¶ä»–éŒ¯èª¤: {e}")
        return False

def check_database_update_time():
    """
    æª¢æŸ¥æœ¬åœ°è³‡æ–™åº«çš„æœ€å¾Œæ›´æ–°æ™‚é–“
    """
    try:
        project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
        local_db_path = os.path.join(project_root, 'data', 'crawler_data.db')
        
        if os.path.exists(local_db_path):
            mtime = os.path.getmtime(local_db_path)
            update_time = datetime.fromtimestamp(mtime)
            print(f"ğŸ“… æœ¬åœ°è³‡æ–™åº«æœ€å¾Œæ›´æ–°æ™‚é–“: {update_time.strftime('%Y-%m-%d %H:%M:%S')}")
            return update_time
        else:
            print("âŒ æœ¬åœ°è³‡æ–™åº«ä¸å­˜åœ¨")
            return None
    except Exception as e:
        print(f"âŒ æª¢æŸ¥è³‡æ–™åº«æ›´æ–°æ™‚é–“å¤±æ•—: {e}")
        return None

def auto_sync_if_needed(max_age_hours=1):
    """
    å¦‚æœæœ¬åœ°è³‡æ–™åº«å¤ªèˆŠï¼Œè‡ªå‹•åŒæ­¥
    """
    try:
        update_time = check_database_update_time()
        
        if update_time is None:
            print("ğŸ”„ æœ¬åœ°è³‡æ–™åº«ä¸å­˜åœ¨ï¼Œé–‹å§‹ä¸‹è¼‰...")
            return download_latest_database()
        
        # æª¢æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        now = datetime.now()
        age_hours = (now - update_time).total_seconds() / 3600
        
        if age_hours > max_age_hours:
            print(f"ğŸ”„ æœ¬åœ°è³‡æ–™åº«å·² {age_hours:.1f} å°æ™‚æœªæ›´æ–°ï¼Œé–‹å§‹åŒæ­¥...")
            return download_latest_database()
        else:
            print(f"âœ… æœ¬åœ°è³‡æ–™åº«å¤ æ–°ï¼ˆ{age_hours:.1f} å°æ™‚å‰ï¼‰ï¼Œç„¡éœ€åŒæ­¥")
            return True
            
    except Exception as e:
        print(f"âŒ è‡ªå‹•åŒæ­¥æª¢æŸ¥å¤±æ•—: {e}")
        return False

if __name__ == "__main__":
    # æ¸¬è©¦åŠŸèƒ½
    print("ğŸ§ª æ¸¬è©¦ GitHub è³‡æ–™åº«åŒæ­¥åŠŸèƒ½...")
    check_database_update_time()
    download_latest_database()
