"""
çˆ¬èŸ²çµæœå±•ç¤ºç¶²é æ‡‰ç”¨
ä½¿ç”¨Flaskå»ºç«‹Webä»‹é¢ä¾†é¡¯ç¤ºå’Œç®¡ç†çˆ¬èŸ²çµæœ
"""

from flask import Flask, render_template, request, jsonify, send_from_directory
import os
import json
import glob
from datetime import datetime
from crawler_manager import CrawlerManager


app = Flask(__name__)

# è¨­å®šéœæ…‹æª”æ¡ˆè·¯å¾‘
app.static_folder = 'static'
app.template_folder = 'templates'

# åˆå§‹åŒ–çˆ¬èŸ²ç®¡ç†å™¨
crawler_manager = CrawlerManager()

# é…ç½®æ‡‰ç”¨ç¨‹å¼



@app.route('/')
def index():
    """ä¸»é é¢"""
    return render_template('index.html')

@app.route('/api/crawlers')
def get_crawlers():
    """ç²å–å¯ç”¨çš„çˆ¬èŸ²åˆ—è¡¨ï¼ˆæ’é™¤æ¯æ—¥ä¿ƒéŠ·çˆ¬èŸ²ï¼‰"""
    all_crawlers = crawler_manager.list_crawlers()
    # æ’é™¤ pchome_onsaleï¼Œå› ç‚ºé€™æ˜¯æ¯æ—¥ä¿ƒéŠ·å°ˆç”¨çš„ï¼Œä¸è®“ç”¨æˆ¶æ‰‹å‹•åŸ·è¡Œ
    available_crawlers = [crawler for crawler in all_crawlers if crawler != 'pchome_onsale']
    
    return jsonify({
        'crawlers': available_crawlers,
        'status': 'success'
    })

@app.route('/api/results')
def get_results():
    """ç²å–æ‰€æœ‰çˆ¬èŸ²çµæœæª”æ¡ˆï¼ˆæ’é™¤æ¯æ—¥ä¿ƒéŠ·å°ˆç”¨æª”æ¡ˆï¼‰"""
    results_dir = crawler_manager.output_dir
    files = []
    
    # æœå°‹æ‰€æœ‰JSONæª”æ¡ˆ
    json_files = glob.glob(os.path.join(results_dir, "*.json"))
    
    for file_path in json_files:
        filename = os.path.basename(file_path)
        
        # æ’é™¤æ¯æ—¥ä¿ƒéŠ·å°ˆç”¨æª”æ¡ˆ
        if 'pchome_onsale' in filename or 'yahoo_rushbuy' in filename:
            continue
            
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            file_info = {
                'filename': os.path.basename(file_path),
                'filepath': file_path,
                'keyword': data.get('keyword', 'unknown'),
                'total_products': data.get('total_products', 0),
                'crawl_time': data.get('crawl_time', ''),
                'file_size': os.path.getsize(file_path),
                'platforms': list(data.get('results', {}).keys()) if 'results' in data else [data.get('platform', 'unknown')]
            }
            files.append(file_info)
        except Exception as e:
            print(f"è®€å–æª”æ¡ˆ {file_path} å¤±æ•—: {e}")
    
    # æŒ‰æ™‚é–“æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
    files.sort(key=lambda x: x['crawl_time'], reverse=True)
    
    return jsonify({
        'files': files,
        'status': 'success'
    })

@app.route('/api/result/<filename>')
def get_result_detail(filename):
    """ç²å–ç‰¹å®šçµæœæª”æ¡ˆçš„è©³ç´°å…§å®¹"""
    file_path = os.path.join(crawler_manager.output_dir, filename)
    
    if not os.path.exists(file_path):
        return jsonify({'error': 'æª”æ¡ˆä¸å­˜åœ¨'}), 404
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        return jsonify({
            'data': data,
            'status': 'success'
        })
    except Exception as e:
        return jsonify({'error': f'è®€å–æª”æ¡ˆå¤±æ•—: {str(e)}'}), 500

@app.route('/api/crawl', methods=['POST'])
def start_crawl():
    """å•Ÿå‹•æ–°çš„çˆ¬èŸ²ä»»å‹™"""
    try:
        data = request.get_json()
        keyword = data.get('keyword', '')
        platforms = data.get('platforms', [])
        max_products = int(data.get('max_products', 100))
        min_price = int(data.get('min_price', 0))
        max_price = int(data.get('max_price', 999999))
        
        if not keyword:
            return jsonify({'error': 'è«‹è¼¸å…¥æœç´¢é—œéµå­—'}), 400
        
        if not platforms:
            platforms = crawler_manager.list_crawlers()
        
        # åŸ·è¡Œçˆ¬èŸ²
        results = crawler_manager.run_all_crawlers(
            keyword=keyword,
            max_products=max_products,
            min_price=min_price,
            max_price=max_price,
            platforms=platforms
        )
        
        # ä¿å­˜çµæœ
        filename = crawler_manager.save_results(keyword, results)
        
        return jsonify({
            'status': 'success',
            'message': 'çˆ¬èŸ²åŸ·è¡Œå®Œæˆ',
            'filename': os.path.basename(filename),
            'results': results
        })
        
    except Exception as e:
        return jsonify({'error': f'çˆ¬èŸ²åŸ·è¡Œå¤±æ•—: {str(e)}'}), 500

@app.route('/api/statistics/<filename>')
def get_statistics(filename):
    """ç²å–ç‰¹å®šçµæœçš„çµ±è¨ˆè³‡è¨Š"""
    file_path = os.path.join(crawler_manager.output_dir, filename)
    
    if not os.path.exists(file_path):
        return jsonify({'error': 'æª”æ¡ˆä¸å­˜åœ¨'}), 404
    
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        # è¨ˆç®—çµ±è¨ˆè³‡è¨Š
        stats = {
            'keyword': data.get('keyword', ''),
            'total_products': 0,
            'platforms': {},
            'price_stats': {
                'min': float('inf'),
                'max': 0,
                'average': 0,
                'total': 0
            }
        }
        
        all_prices = []
        
        # è™•ç†æ–°æ ¼å¼ï¼ˆåŒ…å«å¤šå€‹å¹³å°ï¼‰
        if 'results' in data:
            results = data['results']
            for platform, result in results.items():
                platform_stats = {
                    'product_count': result.get('total_products', 0),
                    'status': result.get('status', 'unknown'),
                    'execution_time': result.get('execution_time', 0)
                }
                stats['platforms'][platform] = platform_stats
                stats['total_products'] += platform_stats['product_count']
                
                # æ”¶é›†åƒ¹æ ¼
                for product in result.get('products', []):
                    price = product.get('price', 0)
                    if price > 0:
                        all_prices.append(price)
        
        # è™•ç†èˆŠæ ¼å¼ï¼ˆå–®ä¸€å¹³å°ï¼‰
        elif 'products' in data:
            platform = data.get('platform', 'unknown')
            stats['platforms'][platform] = {
                'product_count': len(data['products']),
                'status': 'success',
                'execution_time': 0
            }
            stats['total_products'] = len(data['products'])
            
            for product in data['products']:
                price = product.get('price', 0)
                if price > 0:
                    all_prices.append(price)
        
        # è¨ˆç®—åƒ¹æ ¼çµ±è¨ˆ
        if all_prices:
            stats['price_stats']['min'] = min(all_prices)
            stats['price_stats']['max'] = max(all_prices)
            stats['price_stats']['average'] = sum(all_prices) / len(all_prices)
            stats['price_stats']['total'] = len(all_prices)
        else:
            stats['price_stats']['min'] = 0
        
        return jsonify({
            'statistics': stats,
            'status': 'success'
        })
        
    except Exception as e:
        return jsonify({'error': f'çµ±è¨ˆè¨ˆç®—å¤±æ•—: {str(e)}'}), 500

@app.route('/view/<filename>')
def view_result(filename):
    """æŸ¥çœ‹ç‰¹å®šçµæœçš„è©³ç´°é é¢"""
    return render_template('result_detail.html', filename=filename)

@app.route('/crawler')
def crawler_page():
    """çˆ¬èŸ²åŸ·è¡Œé é¢"""
    return render_template('crawler.html')

@app.route('/api/daily-deals')
def get_daily_deals():
    """ç²å–æ¯æ—¥ä¿ƒéŠ·ï¼ˆPCHOME ONSALE + YAHOO RUSHBUYï¼‰çµæœ"""
    results_dir = crawler_manager.output_dir
    daily_deals = []
    
    # è™•ç† PCHOME ONSALE å•†å“
    pchome_file = os.path.join(results_dir, "crawler_results_pchome_onsale.json")
    if os.path.exists(pchome_file):
        try:
            with open(pchome_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            # è™•ç†æ–°æ ¼å¼ï¼ˆæœ‰ results çµæ§‹ï¼‰æˆ–èˆŠæ ¼å¼
            if 'results' in data and 'pchome_onsale' in data['results']:
                # æ–°æ ¼å¼ï¼šæœ‰ results çµæ§‹
                pchome_result = data['results']['pchome_onsale']
                file_info = {
                    'filename': os.path.basename(pchome_file),
                    'filepath': pchome_file,
                    'keyword': data.get('keyword', 'pchome_onsale'),
                    'total_products': pchome_result.get('total_products', 0),
                    'crawl_time': data.get('crawl_time', pchome_result.get('crawl_time', '')),
                    'file_size': os.path.getsize(pchome_file),
                    'platform': 'pchome_onsale',
                    'products': pchome_result.get('products', [])
                }
                daily_deals.append(file_info)
            elif data.get('platform') == 'pchome_onsale' or 'pchome_onsale' in data.get('keyword', ''):
                # èˆŠæ ¼å¼ï¼šç›´æ¥åŒ…å«ç”¢å“è³‡æ–™
                file_info = {
                    'filename': os.path.basename(pchome_file),
                    'filepath': pchome_file,
                    'keyword': data.get('keyword', 'pchome_onsale'),
                    'total_products': data.get('total_products', 0),
                    'crawl_time': data.get('crawl_time', data.get('timestamp', '')),
                    'file_size': os.path.getsize(pchome_file),
                    'platform': 'pchome_onsale',
                    'products': data.get('products', [])
                }
                daily_deals.append(file_info)
        except Exception as e:
            print(f"è®€å– PChome æ¯æ—¥ä¿ƒéŠ·æª”æ¡ˆå¤±æ•—: {e}")
    
    # è™•ç† YAHOO RUSHBUY å•†å“
    yahoo_file = os.path.join(results_dir, "crawler_results_yahoo_rushbuy.json")
    if os.path.exists(yahoo_file):
        try:
            with open(yahoo_file, 'r', encoding='utf-8-sig') as f:
                data = json.load(f)
                
            if data.get('platform') == 'yahoo_rushbuy':
                file_info = {
                    'filename': os.path.basename(yahoo_file),
                    'filepath': yahoo_file,
                    'keyword': data.get('keyword', 'yahoo_rushbuy'),
                    'total_products': data.get('total_products', 0),
                    'crawl_time': data.get('crawl_time', data.get('timestamp', '')),
                    'file_size': os.path.getsize(yahoo_file),
                    'platform': 'yahoo_rushbuy',
                    'products': data.get('products', [])
                }
                daily_deals.append(file_info)
        except Exception as e:
            print(f"è®€å– Yahoo ç§’æ®ºæª”æ¡ˆå¤±æ•—: {e}")    # æŒ‰æ™‚é–“æ’åºï¼ˆæœ€æ–°çš„åœ¨å‰é¢ï¼‰
    daily_deals.sort(key=lambda x: x.get('crawl_time', ''), reverse=True)
    
    # æ”¯æ´å¹³å°éæ¿¾
    platform_filter = request.args.get('platform', 'all')
    if platform_filter != 'all':
        daily_deals = [deal for deal in daily_deals if deal['platform'] == platform_filter]
    
    # å–å¾—å„å¹³å°æœ€å¾Œæ›´æ–°æ™‚é–“
    platform_update_times = {}
    for deal in daily_deals:
        platform = deal['platform']
        if platform not in platform_update_times:
            platform_update_times[platform] = deal.get('crawl_time', '')
    
    return jsonify({
        'daily_deals': daily_deals,
        'total_deals': sum(deal['total_products'] for deal in daily_deals),
        'latest_update': daily_deals[0]['crawl_time'] if daily_deals else '',
        'platform_updates': platform_update_times,
        'next_update_times': {
            'morning': '08:00',
            'afternoon': '14:00',
            'evening': '20:00',
            'night': '02:00'
        },
        'status': 'success'
    })

@app.route('/daily-deals')
def daily_deals_page():
    """æ¯æ—¥ä¿ƒéŠ·å°ˆé """
    return render_template('daily_deals.html')

@app.route('/api/products/compare', methods=['POST'])
def compare_products():
    """å•†å“æ¯”è¼ƒ API"""
    print("=" * 50)
    print("é€²å…¥å•†å“æ¯”è¼ƒ API")
    print("=" * 50)
    
    if not GEMINI_AVAILABLE or not model:
        print("Gemini API ä¸å¯ç”¨")
        return jsonify({
            'error': 'Gemini API æœªé…ç½®ï¼Œç„¡æ³•ä½¿ç”¨å•†å“æ¯”è¼ƒåŠŸèƒ½',
            'similarProducts': []
        }), 503
    
    try:
        data = request.get_json()
        target_product = {
            'title': data.get('productName', ''),
            'platform': data.get('platform', ''),
            'price': data.get('price', 0)
        }
        
        # å¾æ‰€æœ‰çˆ¬èŸ²çµæœä¸­å°‹æ‰¾å€™é¸å•†å“
        candidate_products = []
        results_dir = crawler_manager.output_dir
        json_files = glob.glob(os.path.join(results_dir, "*.json"))
        
        for file_path in json_files:
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    file_data = json.load(f)
                
                # å¾ä¸åŒæ ¼å¼çš„æª”æ¡ˆä¸­æå–å•†å“
                products = extract_products_from_file(file_data)
                
                # ç¬¬ä¸€å±¤éæ¿¾ï¼šä½¿ç”¨æ”¹é€²çš„é—œéµå­—éæ¿¾æ¸›å°‘å€™é¸å•†å“æ•¸é‡
                keywords = extract_keywords(target_product['title'])
                if keywords['core'] or keywords['important']:
                    filtered_products = []
                    for product in products:
                        product_title = product.get('title', product.get('name', ''))
                        product_lower = product_title.lower()
                        
                        # æ›´åš´æ ¼çš„åŒ¹é…æ¢ä»¶
                        match_score = 0
                        total_core = len(keywords['core'])
                        total_important = len(keywords['important'])
                        
                        # æ ¸å¿ƒé—œéµå­—åŒ¹é…ï¼ˆå“ç‰Œã€å‹è™Ÿï¼‰
                        core_matches = sum(1 for kw in keywords['core'] if kw.lower() in product_lower)
                        # é‡è¦é—œéµå­—åŒ¹é…ï¼ˆè¦æ ¼ï¼‰
                        important_matches = sum(1 for kw in keywords['important'] if kw.lower() in product_lower)
                        
                        # è¨ˆç®—åŒ¹é…åˆ†æ•¸
                        if total_core > 0:
                            match_score += (core_matches / total_core) * 0.6  # æ ¸å¿ƒé—œéµå­—æ¬Šé‡60%
                        if total_important > 0:
                            match_score += (important_matches / total_important) * 0.4  # é‡è¦é—œéµå­—æ¬Šé‡40%
                        
                        # åªä¿ç•™åŒ¹é…åˆ†æ•¸ >= 0.5 çš„å•†å“ï¼ˆè‡³å°‘ä¸€åŠé—œéµå­—åŒ¹é…ï¼‰
                        if match_score >= 0.5:
                            filtered_products.append(product)
                    
                    print(f"æª”æ¡ˆ {os.path.basename(file_path)}: é—œéµå­—éæ¿¾")
                    print(f"  æ ¸å¿ƒé—œéµå­—: {keywords['core']}")
                    print(f"  é‡è¦é—œéµå­—: {keywords['important']}")
                    print(f"  éæ¿¾å‰: {len(products)} å€‹å•†å“")
                    print(f"  éæ¿¾å¾Œ: {len(filtered_products)} å€‹å•†å“")
                    
                    candidate_products.extend(filtered_products)
                else:
                    print(f"æª”æ¡ˆ {os.path.basename(file_path)}: ç„¡æœ‰æ•ˆé—œéµå­—ï¼ŒåŠ å…¥æ‰€æœ‰ {len(products)} å€‹å•†å“")
                    candidate_products.extend(products)
                    
            except Exception as e:
                print(f"è®€å–æª”æ¡ˆ {file_path} å¤±æ•—: {e}")
                continue
        
        # ç§»é™¤èˆ‡ç›®æ¨™å•†å“ç›¸åŒçš„å•†å“
        candidate_products = [p for p in candidate_products 
                            if p.get('title', '') != target_product['title'] or 
                               p.get('platform', '') != target_product['platform']]
        
        if not candidate_products:
            return jsonify({
                'similarProducts': [],
                'totalCandidates': 0,
                'totalMatches': 0
            })
        
        print(f"æ‰¾åˆ° {len(candidate_products)} å€‹å€™é¸å•†å“é€²è¡Œæ¯”è¼ƒ")
        
        # é¡¯ç¤ºéæ¿¾å¾Œçš„å€™é¸å•†å“JSON
        print("=" * 60)
        print("éæ¿¾å¾Œçš„å€™é¸å•†å“åˆ—è¡¨ (JSONæ ¼å¼):")
        print("=" * 60)
        candidate_products_json = json.dumps(candidate_products, ensure_ascii=False, indent=2)
        print(candidate_products_json)
        print("=" * 60)
        print(f"å€™é¸å•†å“ç¸½æ•¸: {len(candidate_products)}")
        
        # ä¿å­˜éæ¿¾å¾Œçš„å€™é¸å•†å“åˆ°æª”æ¡ˆ
        # æ¸…ç†æª”æ¡ˆåç¨±ä¸­çš„ç‰¹æ®Šå­—ç¬¦
        clean_title = target_product['title'].replace(' ', '_').replace('/', '_').replace('\\', '_').replace(':', '_').replace('?', '_').replace('*', '_').replace('<', '_').replace('>', '_').replace('|', '_').replace('"', '_')
        filtered_filename = f"filtered_candidates_{clean_title}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        filtered_filepath = os.path.join(crawler_manager.output_dir, filtered_filename)
        
        print(f"ğŸ” æº–å‚™ä¿å­˜éæ¿¾çµæœ...")
        print(f"ğŸ“ ç›®æ¨™ç›®éŒ„: {crawler_manager.output_dir}")
        print(f"ğŸ“„ æª”æ¡ˆåç¨±: {filtered_filename}")
        print(f"ğŸ“ å®Œæ•´è·¯å¾‘: {filtered_filepath}")
        
        try:
            # ç¢ºä¿ç›®éŒ„å­˜åœ¨
            os.makedirs(crawler_manager.output_dir, exist_ok=True)
            print(f"âœ… ç›®éŒ„ç¢ºèªå­˜åœ¨: {os.path.exists(crawler_manager.output_dir)}")
            
            filtered_data = {
                'target_product': target_product,
                'filter_keywords': extract_keywords(target_product['title']),
                'total_candidates': len(candidate_products),
                'filtered_time': datetime.now().isoformat(),
                'note': 'ç¬¬ä¸€å±¤æ”¹é€²é—œéµå­—éæ¿¾å¾Œçš„å€™é¸å•†å“ï¼Œæ¡ç”¨åŠ æ¬ŠåŒ¹é…åˆ†æ•¸â‰¥0.5ï¼Œå°‡äº¤ç”±Gemini AIé€²è¡Œç¬¬äºŒå±¤æ™ºæ…§æ¯”å°',
                'candidates': candidate_products
            }
            
            print(f"ğŸ’¾ é–‹å§‹å¯«å…¥æª”æ¡ˆ...")
            with open(filtered_filepath, 'w', encoding='utf-8') as f:
                json.dump(filtered_data, f, ensure_ascii=False, indent=2)
            
            # é©—è­‰æª”æ¡ˆæ˜¯å¦æˆåŠŸå‰µå»º
            if os.path.exists(filtered_filepath):
                file_size = os.path.getsize(filtered_filepath)
                print(f"âœ… éæ¿¾çµæœå·²æˆåŠŸä¿å­˜!")
                print(f"ğŸ“ æª”æ¡ˆä½ç½®: {filtered_filepath}")
                print(f"ï¿½ æª”æ¡ˆå¤§å°: {file_size} bytes")
            else:
                print(f"âŒ æª”æ¡ˆå‰µå»ºå¤±æ•—ï¼Œæª”æ¡ˆä¸å­˜åœ¨")
            
        except PermissionError as e:
            print(f"âŒ æ¬Šé™éŒ¯èª¤: {e}")
            print(f"ğŸ’¡ è«‹ç¢ºèªå° {crawler_manager.output_dir} ç›®éŒ„æœ‰å¯«å…¥æ¬Šé™")
        except Exception as e:
            print(f"âŒ ä¿å­˜éæ¿¾çµæœå¤±æ•—: {e}")
            print(f"âŒ éŒ¯èª¤é¡å‹: {type(e).__name__}")
            import traceback
            print(f"âŒ è©³ç´°éŒ¯èª¤: {traceback.format_exc()}")
        
        # é¡¯ç¤ºå‰3å€‹å•†å“çš„è©³ç´°ä¿¡æ¯
        for i, product in enumerate(candidate_products[:3]):
            print(f"\nå€™é¸å•†å“ {i+1}:")
            print(f"  - æ¨™é¡Œ: {product.get('title', product.get('name', ''))}")
            print(f"  - å¹³å°: {product.get('platform', '')}")
            print(f"  - åƒ¹æ ¼: ${product.get('price', 0)}")
            print(f"  - URL: {product.get('url', '')}")
        
        if len(candidate_products) > 3:
            print(f"\n... é‚„æœ‰ {len(candidate_products) - 3} å€‹å•†å“")
        print("=" * 60)
        
        # ä½¿ç”¨ Gemini é€²è¡Œå–®æ¬¡æ¯”è¼ƒ
        try:
            matches = product_comparison_service.compare_products(target_product, candidate_products)
            print(f"å–®æ¬¡APIå‘¼å«å®Œæˆï¼Œæ‰¾åˆ° {len(matches)} å€‹åŒ¹é…çµæœ")
        except Exception as e:
            print(f"å•†å“æ¯”è¼ƒAPIå‘¼å«å¤±æ•—: {e}")
            matches = []
        
        # éæ¿¾ä¸¦çµ„ç¹”çµæœ
        similar_products = []
        for match in matches:
            if match.get('similarity', 0) >= 0.75:  # é™ä½é–€æª»åˆ°0.75
                product = candidate_products[match['index']]
                similar_product = {
                    'title': product.get('title', product.get('name', '')),
                    'price': product.get('price', 0),
                    'platform': product.get('platform', ''),
                    'url': product.get('url', ''),
                    'image_url': product.get('image_url', ''),
                    'similarity': match.get('similarity', 0),
                    'reason': match.get('reason', ''),
                    'confidence': match.get('confidence', 'ä¸­'),
                    'category': match.get('category', 'ç›¸ä¼¼å•†å“')
                }
                similar_products.append(similar_product)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similar_products.sort(key=lambda x: x['similarity'], reverse=True)
        
        print(f"æ‰¾åˆ° {len(similar_products)} å€‹ç›¸ä¼¼å•†å“ (â‰¥75%)")
        
        return jsonify({
            'similarProducts': similar_products,
            'totalCandidates': len(candidate_products),
            'totalMatches': len(matches),
            'highQualityMatches': len(similar_products),
            'similarityThreshold': 0.75
        })
        
    except Exception as e:
        print(f"å•†å“æ¯”è¼ƒéŒ¯èª¤: {e}")
        return jsonify({
            'error': f'å•†å“æ¯”è¼ƒå¤±æ•—: {str(e)}',
            'similarProducts': []
        }), 500

def extract_products_from_file(file_data):
    """å¾æª”æ¡ˆè³‡æ–™ä¸­æå–å•†å“åˆ—è¡¨"""
    products = []
    
    # è™•ç†æ–°æ ¼å¼ï¼ˆæœ‰ results çµæ§‹ï¼‰
    if 'results' in file_data:
        for platform, platform_data in file_data['results'].items():
            if 'products' in platform_data:
                for product in platform_data['products']:
                    product['platform'] = platform
                    products.append(product)
    
    # è™•ç†èˆŠæ ¼å¼ï¼ˆç›´æ¥åŒ…å«ç”¢å“è³‡æ–™ï¼‰
    elif 'products' in file_data:
        for product in file_data['products']:
            if 'platform' not in product:
                product['platform'] = file_data.get('platform', 'unknown')
            products.append(product)
    
    return products

def extract_keywords(text):
    """æå–é—œéµå­—ä¸¦æŒ‰é‡è¦æ€§åˆ†é¡ - é€šç”¨ç‰ˆæœ¬é©ç”¨æ–¼å„ç¨®å•†å“é¡å‹"""
    if not text:
        return {'core': [], 'important': [], 'optional': []}
    
    # ç§»é™¤å¸¸è¦‹çš„ç„¡æ„ç¾©è©å½™
    stop_words = ['çš„', 'å’Œ', 'èˆ‡', 'æˆ–', 'åŠ', 'å€‹', 'çµ„', 'å…¥', 'ç›’', 'åŒ…', 'è£', 'ç‰ˆ', 'å‹', 'æ¬¾', 'ä»¶', 'æ¢', 'æ”¯', 'ç‰‡', 'é¡†', 'å°', 'éƒ¨', 'éš»', 'é›™', 'å°']
    
    # é€šç”¨å“ç‰Œé—œéµè©ï¼ˆæ ¸å¿ƒï¼‰- è¦†è“‹å¤šç¨®å•†å“é¡å‹
    core_brands = [
        # 3Cé›»å­å“ç‰Œ
        'Apple', 'iPhone', 'iPad', 'MacBook', 'Samsung', 'LG', 'Sony', 'ASUS', 'MSI', 'GIGABYTE', 'æŠ€å˜‰', 'å¾®æ˜Ÿ', 'è¯ç¢©',
        'Dell', 'HP', 'Lenovo', 'Acer', 'NVIDIA', 'AMD', 'Intel', 'GeForce', 'RTX', 'GTX', 'Radeon', 'Arc',
        # é‹å‹•å“ç‰Œ
        'Nike', 'Adidas', 'PUMA', 'New Balance', 'Converse', 'Vans', 'ASICS', 'Mizuno', 'Under Armour',
        # æ±½è»Šå“ç‰Œ
        'Toyota', 'Honda', 'BMW', 'Mercedes', 'Audi', 'Volkswagen', 'Ford', 'Nissan', 'Mazda', 'Hyundai',
        # å®¶é›»å“ç‰Œ
        'Panasonic', 'Sharp', 'Toshiba', 'Philips', 'Bosch', 'Siemens', 'Whirlpool', 'Electrolux',
        # é£Ÿå“å“ç‰Œ
        'Coca-Cola', 'Pepsi', 'Nestle', 'Unilever', 'P&G', 'Johnson', 'Kraft', 'Kellogg',
        # æ™‚å°šå“ç‰Œ
        'Gucci', 'Louis Vuitton', 'Chanel', 'HermÃ¨s', 'Prada', 'Burberry', 'Coach', 'Michael Kors'
    ]
    
    # é‡è¦è¦æ ¼/å‹è™Ÿé—œéµè© - ä½¿ç”¨æ­£å‰‡è¡¨é”å¼åŒ¹é…æ¨¡å¼
    important_patterns = [
        # æ•¸å­—+å–®ä½çµ„åˆ (å¦‚: 32G, 24GB, 1TB, 500GB, 16å‹, 14inchç­‰)
        r'\d+[GT]B?',  # è¨˜æ†¶é«”/å„²å­˜å®¹é‡
        r'\d+å‹',      # è¢å¹•å°ºå¯¸
        r'\d+inch',    # è‹±å¯¸å°ºå¯¸
        # å‹è™Ÿæ•¸å­— (å¦‚: 5090, 4080, iPhone 15, Galaxy S24ç­‰)
        r'\d{3,4}',    # 3-4ä½æ•¸å­—å‹è™Ÿ
        # å¸¸è¦‹è¦æ ¼è©
        r'Pro|Max|Mini|Plus|Ultra|Super|Ti|Air|Lite'
    ]
    
    # å•†å“é¡å‹é—œéµè©ï¼ˆé‡è¦ï¼‰
    product_types = [
        'é¡¯ç¤ºå¡', 'é¡¯å¡', 'ä¸»æ©Ÿæ¿', 'ä¸»æ¿', 'è™•ç†å™¨', 'CPU', 'GPU', 'è¨˜æ†¶é«”', 'ç¡¬ç¢Ÿ', 'SSD', 'HDD',
        'æ‰‹æ©Ÿ', 'å¹³æ¿', 'ç­†é›»', 'ç­†è¨˜å‹é›»è…¦', 'æ¡Œæ©Ÿ', 'è¢å¹•', 'éµç›¤', 'æ»‘é¼ ', 'è€³æ©Ÿ', 'å–‡å­',
        'çƒé‹', 'é‹å‹•é‹', 'ç±ƒçƒé‹', 'è·‘é‹', 'ä¼‘é–’é‹', 'æ¶¼é‹', 'é´å­', 'æœé£¾', 'ä¸Šè¡£', 'è¤²å­',
        'å†°ç®±', 'æ´—è¡£æ©Ÿ', 'å†·æ°£', 'é›»è¦–', 'çƒ¤ç®±', 'å¾®æ³¢çˆ', 'å¸å¡µå™¨', 'é™¤æ¿•æ©Ÿ', 'ç©ºæ°£æ¸…æ·¨æ©Ÿ',
        'æ±½è»Š', 'æ©Ÿè»Š', 'è‡ªè¡Œè»Š', 'è¼ªèƒ', 'æ©Ÿæ²¹', 'é›¶ä»¶', 'é…ä»¶'
    ]
    
    # åˆ†å‰²æ–‡å­—ä¸¦åˆ†é¡
    keywords = {'core': [], 'important': [], 'optional': []}
    words = re.split(r'[\s,ï¼Œã€‚()ï¼ˆï¼‰\[\]ã€ã€‘\-_/\\]+', text)
    
    for word in words:
        word = word.strip()
        if len(word) <= 1 or word in stop_words:
            continue
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºæ ¸å¿ƒå“ç‰Œ
        if any(brand.lower() == word.lower() for brand in core_brands):
            keywords['core'].append(word)
        # æª¢æŸ¥æ˜¯å¦ç¬¦åˆé‡è¦è¦æ ¼æ¨¡å¼
        elif any(re.match(pattern, word, re.IGNORECASE) for pattern in important_patterns):
            keywords['important'].append(word)
        # æª¢æŸ¥æ˜¯å¦ç‚ºå•†å“é¡å‹
        elif any(ptype.lower() in word.lower() for ptype in product_types):
            keywords['important'].append(word)
        # æª¢æŸ¥æ˜¯å¦ç‚ºç´”æ•¸å­—ï¼ˆå¯èƒ½æ˜¯é‡è¦å‹è™Ÿï¼‰
        elif word.isdigit() and len(word) >= 2:
            keywords['important'].append(word)
        # å…¶ä»–æœ‰æ„ç¾©çš„è©å½™
        elif len(word) >= 2 and not word.isdigit():
            keywords['optional'].append(word)
    
    # å»é‡ä¸¦é™åˆ¶æ¯é¡é—œéµå­—æ•¸é‡
    keywords['core'] = list(dict.fromkeys(keywords['core']))[:3]  # æœ€å¤š3å€‹æ ¸å¿ƒé—œéµå­—
    keywords['important'] = list(dict.fromkeys(keywords['important']))[:4]  # æœ€å¤š4å€‹é‡è¦é—œéµå­—
    keywords['optional'] = list(dict.fromkeys(keywords['optional']))[:2]  # æœ€å¤š2å€‹å¯é¸é—œéµå­—
    
    return keywords

# æ‰‹å‹•æ›´æ–° API å·²ç§»é™¤ï¼Œä½¿ç”¨ GitHub Actions è‡ªå‹•æ›´æ–°

if __name__ == '__main__':
    # ç¢ºä¿templateså’Œstaticç›®éŒ„å­˜åœ¨
    os.makedirs('templates', exist_ok=True)
    os.makedirs('static', exist_ok=True)
    
    print("çˆ¬èŸ²çµæœå±•ç¤ºç¶²ç«™å•Ÿå‹•ä¸­...")
    print("è«‹è¨ªå•: http://localhost:5000")
    
    app.run(debug=True, host='0.0.0.0', port=5000)
